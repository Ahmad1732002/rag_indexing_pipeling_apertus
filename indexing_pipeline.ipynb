{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETH Zurich Web Archive Indexing Pipeline\n",
    "\n",
    "This notebook walks through the complete pipeline for processing WARC files and indexing them to Elasticsearch.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The pipeline consists of 5 main steps:\n",
    "1. **Extract** HTML and PDF files from WARC archives\n",
    "2. **Combine** HTML files by domain and timestamp\n",
    "3. **Convert** HTML to Markdown format\n",
    "4. **Index** documents to Elasticsearch with embeddings\n",
    "5. **Query** the indexed documents\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ollama running locally: `ollama serve`\n",
    "- Embedding model downloaded: `ollama pull all-minilm`\n",
    "- `.env` file configured with Elasticsearch credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup Environment\n",
    "\n",
    "Load environment variables and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  ES URL: https://es.swissai.cscs.ch\n",
      "  ES User: lsaie-1\n",
      "  Embedding Model: all-minilm\n",
      "  Index Name: ethz_webarchive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Read Elasticsearch configuration from .env file\n",
    "es_username = os.getenv('ELASTIC_USERNAME')\n",
    "es_password = os.getenv('ELASTIC_PASSWORD')\n",
    "es_url = os.getenv('ES_URL', 'https://es.swissai.cscs.ch')\n",
    "embedding_model = os.getenv('EMBEDDING_MODEL', 'all-minilm')\n",
    "index_name = os.getenv('INDEX_NAME', 'ethz_webarchive')\n",
    "\n",
    "# Validate credentials\n",
    "if not es_username or not es_password:\n",
    "    raise ValueError(\"Please set ELASTIC_USERNAME and ELASTIC_PASSWORD in your .env file\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  ES URL: {es_url}\")\n",
    "print(f\"  ES User: {es_username}\")\n",
    "print(f\"  Embedding Model: {embedding_model}\")\n",
    "print(f\"  Index Name: {index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract HTML and PDF from WARC Files\n",
    "\n",
    "Extract content from WARC archives. This step:\n",
    "- Parses WARC files in the data directory\n",
    "- Extracts HTML pages (for text content)\n",
    "- Extracts PDF files (for document content)\n",
    "- Organizes files by domain\n",
    "\n",
    "**Note:** We clean the output directory first to ensure a fresh start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned output directory\n",
      "\n",
      "Extracting HTML and PDF files from WARC archives...\n",
      "\n",
      "Processing collection: 19945\n",
      "  Extracting HTML...\n",
      "parsing ARCHIVEIT-19945-WEEKLY-JOB2538523-SEED3004629-20250410105708162-00000-h3.warc.gz\n",
      "parsing ARCHIVEIT-19945-TEST-JOB2537999-0-SEED4432726-20250409125132954-00000-uhxaspzf.warc.gz\n",
      "parsing ARCHIVEIT-19945-WEEKLY-JOB2538523-SEED3004637-20250410105648171-00000-h3.warc.gz\n",
      "parsing ARCHIVEIT-19945-TEST-JOB2538000-0-SEED4432727-20250409125201867-00000-9618ziof.warc.gz\n",
      "-----------------------------\n",
      "\n",
      "Count of records.\n",
      "59\n",
      "\n",
      "Count of types.\n",
      "{'response': 59}\n",
      "\n",
      "Count of warc-content.\n",
      "{'application/http': 59}\n",
      "\n",
      "Count of http-content.\n",
      "{'text/html': 59}\n",
      "\n",
      "Count of status.\n",
      "{'200': 52, '404': 6, '500': 1}\n",
      "  Extracting PDFs...\n",
      "parsing ARCHIVEIT-19945-WEEKLY-JOB2538523-SEED3004629-20250410105708162-00000-h3.warc.gz\n",
      "parsing ARCHIVEIT-19945-TEST-JOB2537999-0-SEED4432726-20250409125132954-00000-uhxaspzf.warc.gz\n",
      "parsing ARCHIVEIT-19945-WEEKLY-JOB2538523-SEED3004637-20250410105648171-00000-h3.warc.gz\n",
      "parsing ARCHIVEIT-19945-TEST-JOB2538000-0-SEED4432727-20250409125201867-00000-9618ziof.warc.gz\n",
      "-----------------------------\n",
      "\n",
      "Count of records.\n",
      "5\n",
      "\n",
      "Count of types.\n",
      "{'response': 5}\n",
      "\n",
      "Count of warc-content.\n",
      "{'application/http': 5}\n",
      "\n",
      "Count of http-content.\n",
      "{'application/pdf': 5}\n",
      "\n",
      "Count of status.\n",
      "{'200': 5}\n",
      "  ✓ Completed collection 19945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prep_warc_files import warc_to_html, warc_to_pdf\n",
    "\n",
    "# Clean output directory\n",
    "if os.path.exists(\"output\"):\n",
    "    shutil.rmtree(\"output\")\n",
    "    print(\"Cleaned output directory\")\n",
    "\n",
    "# Define collections to process\n",
    "coll_list = [\"19945\"]\n",
    "\n",
    "print(\"\\nExtracting HTML and PDF files from WARC archives...\\n\")\n",
    "\n",
    "for coll in coll_list:\n",
    "    print(f\"Processing collection: {coll}\")\n",
    "    \n",
    "    # Extract HTML files\n",
    "    print(\"  Extracting HTML...\")\n",
    "    warc_to_html(\"./data/ethz_websites_2022-2025_examples\", f\"output/html_raw/{coll}/\")\n",
    "    \n",
    "    # Extract PDF files\n",
    "    print(\"  Extracting PDFs...\")\n",
    "    warc_to_pdf(\"./data/ethz_websites_2022-2025_examples\", f\"output/pdf_raw/{coll}/\")\n",
    "    \n",
    "    print(f\"  ✓ Completed collection {coll}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Combine HTML Files by Domain\n",
    "\n",
    "Combine HTML files that belong to the same domain. This step:\n",
    "- Groups files by domain (e.g., all files from ethz.ch)\n",
    "- Keeps the latest version of each page based on timestamp\n",
    "- Creates a mapping file for domain-to-URL conversion\n",
    "- Saves timestamps for each file (used later for metadata)\n",
    "\n",
    "This is important for:\n",
    "- Avoiding duplicate content\n",
    "- Tracking when each page was archived\n",
    "- Organizing content by source domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining HTML files by domain...\n",
      "\n",
      "======================================================================\n",
      "Domain HTML Combiner\n",
      "======================================================================\n",
      "Input:  output/html_raw/19945\n",
      "Output: output/html_combined/19945\n",
      "======================================================================\n",
      "Found: geosynklinale.ch - 2025-04-09 12:51:32 - ARCHIVEIT-19945-TEST-JOB2537999-0-SEED4432726-20250409125132954-00000-uhxaspzf.warc.gz_geosynklinale.ch\n",
      "Found: ethz.ch - 2025-04-10 10:56:48 - ARCHIVEIT-19945-WEEKLY-JOB2538523-SEED3004637-20250410105648171-00000-h3.warc.gz_ethz.ch\n",
      "Found: youtube.com - 2025-04-09 12:52:01 - ARCHIVEIT-19945-TEST-JOB2538000-0-SEED4432727-20250409125201867-00000-9618ziof.warc.gz_youtube.com\n",
      "Found: ethz.ch - 2025-04-10 10:57:08 - ARCHIVEIT-19945-WEEKLY-JOB2538523-SEED3004629-20250410105708162-00000-h3.warc.gz_ethz.ch\n",
      "Found: hytac.arch.ethz.ch - 2025-04-09 12:52:01 - ARCHIVEIT-19945-TEST-JOB2538000-0-SEED4432727-20250409125201867-00000-9618ziof.warc.gz_hytac.arch.ethz.ch\n",
      "Found: geolocation.onetrust.com - 2025-04-10 10:56:48 - ARCHIVEIT-19945-WEEKLY-JOB2538523-SEED3004637-20250410105648171-00000-h3.warc.gz_geolocation.onetrust.com\n",
      "\n",
      "\n",
      "Found 5 unique domains\n",
      "======================================================================\n",
      "\n",
      "Processing domain: ethz.ch\n",
      "  Found 2 folders\n",
      "  + de-it.html (from 2025-04-10 10:56:48)\n",
      "  + de-fr.html (from 2025-04-10 10:56:48)\n",
      "  + de.html (from 2025-04-10 10:56:48)\n",
      "  + de/footer/impressum-fr.html (from 2025-04-10 10:56:48)\n",
      "  + de/footer/datenschutz-it.html (from 2025-04-10 10:56:48)\n",
      "  + de/footer/datenschutz-fr.html (from 2025-04-10 10:56:48)\n",
      "  + de/footer/impressum-it.html (from 2025-04-10 10:56:48)\n",
      "  + de/utils/kontakt-it.html (from 2025-04-10 10:56:48)\n",
      "  + de/utils/kontakt-fr.html (from 2025-04-10 10:56:48)\n",
      "  + en.html (from 2025-04-10 10:57:08)\n",
      "\n",
      "  Copying 10 files to output/html_combined/19945/ethz.ch\n",
      "  ✓ Completed ethz.ch\n",
      "\n",
      "Processing domain: geolocation.onetrust.com\n",
      "  Found 1 folders\n",
      "  + robots.html (from 2025-04-10 10:56:48)\n",
      "\n",
      "  Copying 1 files to output/html_combined/19945/geolocation.onetrust.com\n",
      "  ✓ Completed geolocation.onetrust.com\n",
      "\n",
      "Processing domain: geosynklinale.ch\n",
      "  Found 1 folders\n",
      "  + index.html.gz (from 2025-04-09 12:51:32)\n",
      "  + archiv/index.html.gz (from 2025-04-09 12:51:32)\n",
      "  + kontakt/index.html.gz (from 2025-04-09 12:51:32)\n",
      "  + impressum/index.html.gz (from 2025-04-09 12:51:32)\n",
      "  + naechste-geosynklinale/index.html.gz (from 2025-04-09 12:51:32)\n",
      "  + adressen/index.html.gz (from 2025-04-09 12:51:32)\n",
      "\n",
      "  Copying 6 files to output/html_combined/19945/geosynklinale.ch\n",
      "  ✓ Completed geosynklinale.ch\n",
      "\n",
      "Processing domain: hytac.arch.ethz.ch\n",
      "  Found 1 folders\n",
      "  + index.html (from 2025-04-09 12:52:01)\n",
      "  + elective-25/index.html (from 2025-04-09 12:52:01)\n",
      "  + category/seminarweek/index.html (from 2025-04-09 12:52:01)\n",
      "  + category/collaborations/index.html (from 2025-04-09 12:52:01)\n",
      "  + category/exhibitions/index.html (from 2025-04-09 12:52:01)\n",
      "  + category/news/index.html (from 2025-04-09 12:52:01)\n",
      "  + category/elective/index.html (from 2025-04-09 12:52:01)\n",
      "  + seminarweek/seminar-week-fs18/index.html (from 2025-04-09 12:52:01)\n",
      "  + seminarweek/seminar-week-fs18-2/index.html (from 2025-04-09 12:52:01)\n",
      "  + seminarweek/october-2021-seminar-week/index.html (from 2025-04-09 12:52:01)\n",
      "  + seminarweek/2108/index.html (from 2025-04-09 12:52:01)\n",
      "  + seminarweek/seminar-week-fs22/index.html (from 2025-04-09 12:52:01)\n",
      "  + seminarweek/seminar-week-hs22/index.html (from 2025-04-09 12:52:01)\n",
      "  + seminarweek/seminar-week-hs23/index.html (from 2025-04-09 12:52:01)\n",
      "  + hytac-basics-fs25/index.html (from 2025-04-09 12:52:01)\n",
      "  + author/fanny/index.html (from 2025-04-09 12:52:01)\n",
      "  + collaborations/aletsch-glacier/index.html (from 2025-04-09 12:52:01)\n",
      "  + collaborations/were-kannt-de-willy/index.html (from 2025-04-09 12:52:01)\n",
      "  + collaborations/fall-semester-2021-b-dillenburger-2/index.html (from 2025-04-09 12:52:01)\n",
      "  + collaborations/fall-semester-2021-b-dillenburger/index.html (from 2025-04-09 12:52:01)\n",
      "  + collaborations/collaboration-with-professor-andrea-deplazes-and-idc-ag/index.html (from 2025-04-09 12:52:01)\n",
      "  + collaborations/fall-semester-2021-b-dillenburger-2-2/index.html (from 2025-04-09 12:52:01)\n",
      "  + collaborations/fall-semester-2021-b-dillenburger-2-2-2/index.html (from 2025-04-09 12:52:01)\n",
      "  + collaborations/fall-semester-2021-b-dillenburger-2-2-2-2/index.html (from 2025-04-09 12:52:01)\n",
      "  + exhibitions/exhibition-farewell/index.html (from 2025-04-09 12:52:01)\n",
      "  + exhibitions/exhibition-hs23/index.html (from 2025-04-09 12:52:01)\n",
      "  + exhibitions/exhibition-hs22/index.html (from 2025-04-09 12:52:01)\n",
      "  + exhibitions/exhibition-fs22/index.html (from 2025-04-09 12:52:01)\n",
      "  + about-2/index.html (from 2025-04-09 12:52:01)\n",
      "  + news/sicco-ideal-architecture-house/index.html (from 2025-04-09 12:52:01)\n",
      "  + news/clay-3d-printing-for-architectural-design/index.html (from 2025-04-09 12:52:01)\n",
      "  + showcase/index.html (from 2025-04-09 12:52:01)\n",
      "  + showcase/collaborations-posts/index.html (from 2025-04-09 12:52:01)\n",
      "  + showcase/exhibitions-posts/index.html (from 2025-04-09 12:52:01)\n",
      "  + showcase/seminarweek-posts/index.html (from 2025-04-09 12:52:01)\n",
      "  + showcase/elective-posts/index.html (from 2025-04-09 12:52:01)\n",
      "  + elective/september-2022-soglio-scan/index.html (from 2025-04-09 12:52:01)\n",
      "  + elective/july-2022-elective-course-winter-semester-2021/index.html (from 2025-04-09 12:52:01)\n",
      "  + elective/july-2022-elective-course-summer-semester-2022/index.html (from 2025-04-09 12:52:01)\n",
      "  + helpdesk/index.html (from 2025-04-09 12:52:01)\n",
      "\n",
      "  Copying 40 files to output/html_combined/19945/hytac.arch.ethz.ch\n",
      "  ✓ Completed hytac.arch.ethz.ch\n",
      "\n",
      "Processing domain: youtube.com\n",
      "  Found 1 folders\n",
      "  + embed/ZTAFQfAKfoQ/index.html(1).gz (from 2025-04-09 12:52:01)\n",
      "  + embed/ZTAFQfAKfoQ/index.html.gz (from 2025-04-09 12:52:01)\n",
      "\n",
      "  Copying 2 files to output/html_combined/19945/youtube.com\n",
      "  ✓ Completed youtube.com\n",
      "\n",
      "======================================================================\n",
      "✓ Successfully combined 5 domains\n",
      "✓ Total files: 59\n",
      "✓ Output directory: output/html_combined/19945\n",
      "✓ Timestamps saved to: output/mappings/19945/timestamps.json\n",
      "======================================================================\n",
      "\n",
      "✓ Collection 19945 Summary:\n",
      "  Processed 5 domains\n",
      "  Total files: 59\n",
      "  Domains: ethz.ch, geolocation.onetrust.com, geosynklinale.ch, hytac.arch.ethz.ch, youtube.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from combine_domains import combine_domains_by_timestamp\n",
    "\n",
    "print(\"Combining HTML files by domain...\\n\")\n",
    "\n",
    "for coll in coll_list:\n",
    "    result = combine_domains_by_timestamp(\n",
    "        input_dir=f\"output/html_raw/{coll}\",\n",
    "        output_dir=f\"output/html_combined/{coll}\",\n",
    "        timestamps_json_path=f\"output/mappings/{coll}/timestamps.json\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✓ Collection {coll} Summary:\")\n",
    "    print(f\"  Processed {result['domains_count']} domains\")\n",
    "    print(f\"  Total files: {result['total_files']}\")\n",
    "    print(f\"  Domains: {', '.join(result['domains'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert HTML to Markdown\n",
    "\n",
    "Convert HTML files to clean Markdown format. This step:\n",
    "- Parses HTML and extracts text content\n",
    "- Converts to Markdown for better text processing\n",
    "- Optionally filters domains based on Excel file (topics mapping)\n",
    "- Creates domain mappings (domain → original URL)\n",
    "\n",
    "**Excel filtering:** If you provide an Excel file with allowed domains, only those domains will be processed. This is useful for focusing on specific content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting HTML to Markdown...\n",
      "\n",
      "======================================================================\n",
      "HTML to Markdown Converter\n",
      "======================================================================\n",
      "Input:  output/html_combined/19945\n",
      "Output: output/markdown/19945\n",
      "Excel:  data/2025-11-20_19945_topics.xlsx\n",
      "======================================================================\n",
      "Loaded 3 allowed domains from Excel\n",
      "Saved domain mappings to output/mappings/19945/domain_mappings.json\n",
      "\n",
      "Processing 5 domain folders...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing domains: 100%|██████████| 5/5 [00:00<00:00, 24.48domain/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "✓ Processed 3 domains\n",
      "✓ Converted 50 files\n",
      "✓ Skipped 6 files\n",
      "✓ Output directory: output/markdown/19945\n",
      "======================================================================\n",
      "\n",
      "✓ Collection 19945 completed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from html_combined_to_markdown import convert_html_combined_to_markdown\n",
    "\n",
    "print(\"Converting HTML to Markdown...\\n\")\n",
    "\n",
    "for coll in coll_list:\n",
    "    result = convert_html_combined_to_markdown(\n",
    "        input_dir=f\"output/html_combined/{coll}\",\n",
    "        output_dir=f\"output/markdown/{coll}\",\n",
    "        excel_path=\"data/2025-11-20_19945_topics.xlsx\",  # Optional: filter domains\n",
    "        mappings_path=f\"output/mappings/{coll}/domain_mappings.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Collection {coll} completed\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Index to Elasticsearch\n",
    "\n",
    "Index the Markdown documents to Elasticsearch with embeddings. This step:\n",
    "- Reads Markdown files with metadata\n",
    "- Splits documents into chunks (for better retrieval)\n",
    "- Generates embeddings using Ollama (local embedding model)\n",
    "- Stores documents and embeddings in Elasticsearch\n",
    "\n",
    "**Metadata included:**\n",
    "- Original URL\n",
    "- Domain\n",
    "- Retrieval timestamp\n",
    "- Page title\n",
    "\n",
    "**Performance notes:**\n",
    "- Documents are processed in batches of 10\n",
    "- This may take several minutes depending on document count\n",
    "- Progress is shown for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents to Elasticsearch...\n",
      "\n",
      "======================================================================\n",
      "Elasticsearch Indexing Pipeline\n",
      "======================================================================\n",
      "Markdown dir: output/markdown/19945\n",
      "Index name:   ethz_webarchive\n",
      "ES URL:       https://es.swissai.cscs.ch\n",
      "Embedding:    all-minilm\n",
      "======================================================================\n",
      "Deleted existing index: ethz_webarchive\n",
      "Loaded 3 domain mappings\n",
      "Loaded timestamps for 59 files\n",
      "\n",
      "Loading markdown documents...\n",
      "Found 50 markdown files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading markdown files: 100%|██████████| 50/50 [00:00<00:00, 3088.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 documents\n",
      "Saved 50 documents to output/ethz_webarchive_documents.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Ollama embedding model...\n",
      "✓ Connected to Ollama with model: all-minilm\n",
      "\n",
      "Processing 50 documents...\n",
      "This may take a while depending on document size and embedding model...\n",
      "\n",
      "Processing batch 1/5 (10 documents)...\n",
      "✓ Batch 1 completed successfully (10/50 documents)\n",
      "\n",
      "Processing batch 2/5 (10 documents)...\n",
      "✓ Batch 2 completed successfully (20/50 documents)\n",
      "\n",
      "Processing batch 3/5 (10 documents)...\n",
      "✓ Batch 3 completed successfully (30/50 documents)\n",
      "\n",
      "Processing batch 4/5 (10 documents)...\n",
      "✓ Batch 4 completed successfully (40/50 documents)\n",
      "\n",
      "Processing batch 5/5 (10 documents)...\n",
      "✓ Batch 5 completed successfully (50/50 documents)\n",
      "\n",
      "======================================================================\n",
      "✓ Successfully indexed 50 documents\n",
      "✓ Index name: ethz_webarchive\n",
      "✓ Elasticsearch URL: https://es.swissai.cscs.ch\n",
      "======================================================================\n",
      "\n",
      "✓ Successfully indexed 50 documents\n",
      "✓ Index: ethz_webarchive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from index_to_elasticsearch import index_markdown_to_elasticsearch\n",
    "\n",
    "print(\"Indexing documents to Elasticsearch...\\n\")\n",
    "\n",
    "for coll in coll_list:\n",
    "    result = index_markdown_to_elasticsearch(\n",
    "        clean_index=True,  # Delete existing index first\n",
    "        es_user=es_username,\n",
    "        es_password=es_password,\n",
    "        es_url=es_url,\n",
    "        embedding_model=embedding_model,\n",
    "        markdown_dir=f\"output/markdown/{coll}\",\n",
    "        index_name=index_name,\n",
    "        mappings_path=f\"output/mappings/{coll}/domain_mappings.json\",\n",
    "        timestamps_path=f\"output/mappings/{coll}/timestamps.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Successfully indexed {result['documents_indexed']} documents\")\n",
    "    print(f\"✓ Index: {result['index_name']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query the Index\n",
    "\n",
    "Now that documents are indexed, you can query them using semantic search.\n",
    "\n",
    "The search:\n",
    "- Converts your query to an embedding\n",
    "- Finds the most similar documents using vector similarity\n",
    "- Returns results with metadata (URL, domain, retrieval date)\n",
    "\n",
    "**Try different queries to explore the indexed content!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_elasticsearch import simple_search, print_search_results\n",
    "\n",
    "# Example query\n",
    "query = \"Was ist die Geosynklinale?\"\n",
    "top_k = 3\n",
    "\n",
    "print(f\"Searching for: '{query}'\\n\")\n",
    "\n",
    "results = simple_search(\n",
    "    query=query,\n",
    "    index_name=index_name,\n",
    "    es_url=es_url,\n",
    "    embedding_model=embedding_model,\n",
    "    top_k=top_k\n",
    ")\n",
    "\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Search Results\n",
    "\n",
    "View the raw search results as a Python list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first result in detail\n",
    "if results:\n",
    "    print(\"First result:\")\n",
    "    for key, value in results[0].items():\n",
    "        if key == 'text':\n",
    "            print(f\"  {key}: {value[:200]}...\")  # Truncate long text\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Your Own Queries\n",
    "\n",
    "Modify the cell below to search for different topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Found 5 results\n",
      "======================================================================\n",
      "\n",
      "[1] index\n",
      "    URL: https://geosynklinale.ch/index.html\n",
      "    URL Preview (browser-friendly): https://geosynklinale.ch\n",
      "    Domain: geosynklinale.ch\n",
      "    Retrieved: None\n",
      "    Score: 1.0000\n",
      "    Preview: \\\\n\\\\n\\\\n\\\\n\n",
      "\n",
      "Seit vielen Jahren wird die Geosynklinale als ein Weihnachtsessen im Dezember organisiert.\n",
      "\n",
      "\\\\n\\\\n\\\\n\\\\n\n",
      "\n",
      "![](https://live.geosynklinale...\n",
      "\n",
      "[2] index\n",
      "    URL: https://geosynklinale.ch/archiv/index.html\n",
      "    URL Preview (browser-friendly): https://geosynklinale.ch/archiv\n",
      "    Domain: geosynklinale.ch\n",
      "    Retrieved: None\n",
      "    Score: 0.5622\n",
      "    Preview: \\\\n\\\\n\\\\n\\\\n\n",
      "\n",
      "Hier sind die Geosynklinale\\-Einladungen aus den letzten paar Jahren.\n",
      "\n",
      "\\\\n\\\\n\\\\n\\\\n\n",
      "\n",
      "---\n",
      "\\\\n\\\\n\\\\n\\\\n\n",
      "\n",
      "2024\n",
      "----\n",
      "\n",
      "\\\\n\\\\n\\\\n\\\\n\n",
      "\n",
      "![Einlad...\n",
      "\n",
      "[3] index\n",
      "    URL: https://geosynklinale.ch/index.html\n",
      "    URL Preview (browser-friendly): https://geosynklinale.ch\n",
      "    Domain: geosynklinale.ch\n",
      "    Retrieved: None\n",
      "    Score: 0.3101\n",
      "    Preview: ch/naechste-geosynklinale/)\n",
      "\n",
      "\\\\n* [Archiv](https://geosynklinale.ch/archiv/)\n",
      "\n",
      "\\\\n* [Adress\\\\xc3\\\\xa4nderung und Registrierung](https://geosynklinale.c...\n",
      "\n",
      "[4] index\n",
      "    URL: https://geosynklinale.ch/naechste-geosynklinale/index.html\n",
      "    URL Preview (browser-friendly): https://geosynklinale.ch/naechste-geosynklinale\n",
      "    Domain: geosynklinale.ch\n",
      "    Retrieved: None\n",
      "    Score: 0.2922\n",
      "    Preview: ch/archiv/)\n",
      "\\\\n* [Adress\\\\xc3\\\\xa4nderung und Registrierung](https://geosynklinale.ch/adressen/)\n",
      "\\\\n* [Kontakt](https://geosynklinale.ch/kontakt/)\n",
      "\\\\n...\n",
      "\n",
      "[5] index\n",
      "    URL: https://geosynklinale.ch/index.html\n",
      "    URL Preview (browser-friendly): https://geosynklinale.ch\n",
      "    Domain: geosynklinale.ch\n",
      "    Retrieved: None\n",
      "    Preview: t\\\\t\\\\t\n",
      "\n",
      "\\\\n\\\\n\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\n",
      "\n",
      "\\\\n\\\\n\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t* [N\\\\xc3\\\\xa4chste Geosynklinale](https://geosynklinale.ch/naechste-geosynklin...\n"
     ]
    }
   ],
   "source": [
    "from query_elasticsearch import simple_search, print_search_results\n",
    "\n",
    "# Try your own query here!\n",
    "my_query = \"Was ist die Geosynklinale?\"\n",
    "my_top_k = 5\n",
    "\n",
    "my_results = simple_search(\n",
    "    query=my_query,\n",
    "    index_name=index_name,\n",
    "    es_url=es_url,\n",
    "    embedding_model=embedding_model,\n",
    "    top_k=my_top_k,\n",
    "    es_user=es_username,\n",
    "    es_password=es_password\n",
    ")\n",
    "\n",
    "print_search_results(my_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
